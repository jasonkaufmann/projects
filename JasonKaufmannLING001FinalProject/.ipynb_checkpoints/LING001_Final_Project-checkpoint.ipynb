{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LING001 Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vector Space Model for information retrieval is an interesting linguistic analysis technique. The paramount issue with language processing is representing words mathematically to do relevant computations. So, the Vector Space Model was developed to allow words to be represented as vectors! Using the model, we can find the similarity between a document and a query, which will tell us the relative importance of the query to the document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think a good corpus to use is the Harry Potter books since they are a good length and I find them interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "- SORCERER STONE\n",
    "- CHAMBER OF SECRETS\n",
    "- PRISONER OF AZKABAN\t\n",
    "- GOBLET OF FIRE\n",
    "- ORDER OF THE PHOENIX\t\n",
    "- HALF-BLOOD PRINCE\t\n",
    "- DEATHLY HALLOWS (PART 1 AND 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question/Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the cosine similarity of each of the main character’s names in all of the Harry Potter books relate to the amount of screen time they have in the Harry Potter movies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the name of the character has a higher cosine similarity, then the weight of the name in the corpus is higher, meaning the importance of the person to the plot is greater, thus, they should have more screen time in the movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin using the model, let's explore why the model may make sense to use in the first place. What makes it work so well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf's law states that the rank of an entity - such as word - in a set will be inversely proportional to the number of times the entity shows up. For instance, in English, the most popular word - \"the\" - accounts for roughly 6% of all the words we say. By Zipf's law, the second most common word - \"of\" - will occur half as often as \"the\", which it mysteriously does, and so on for all subsequent words. This property seems to permeate all of nature and is even built into how we think. However, even though there have been many research papers studying Zipf's Law, no conclusive evidence has been discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"middle\">\n",
       "<img width=\"75%\" src=\"zipf.png\">\n",
       "</img></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<img width=\"75%\" src=\"zipf.png\">\n",
    "</img></div>\"\"\")\n",
    "#Soure = https://www.ahsthenest.com/archive/2018/03/19/zipfs-law/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$number\\hspace{1.5mm}of \\hspace{1.5mm}occurences=\\frac{1}{rank}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, this means that a majority of the words we write and say do not have much meaning. They are only there to space out and connect the meaningful/important concepts. This is why the Vector Space Model works so well. Even if a word appears hundreds of thousands of times in a corpus, it's importance will be diminished. In the Harry Potter books, the word \"the\" occurs all the time, but it gives no information about the plot of the books. The Vector Space Model takes advantage of the Zipf-ian nature of language to extract important terms from a corpus. Below, we use the Vector Space Model to do exactly that. In addition, we show that the Harry Potter books do in fact follow a Pareto distribution, which mathematicallly represent Zipf's law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "[ (3,1) x5,y5 ]  [ (3,2) x6,y6 ]\n",
      "[ (4,1) x7,y7           -      ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jskauf/119.embed\" height=\"1000px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequency of terms in Sorcer of Stone\n",
    "import re\n",
    "from operator import itemgetter \n",
    "from plotly import tools\n",
    "\n",
    "datas=[]\n",
    "layouts=[]\n",
    "for i in range(7):\n",
    "    file = open(f\"B{i+1}.txt\", 'rt')\n",
    "    text = file.read()\n",
    "    text=nltk.word_tokenize(text)\n",
    "    frequency={}\n",
    "    for word in text:\n",
    "        if word.isalpha():\n",
    "            count = frequency.get(word,0)\n",
    "            frequency[word] = count + 1\n",
    "\n",
    "    frequency = sorted(frequency.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    words = [item[0] for item in frequency]\n",
    "    times = [item[1] for item in frequency]\n",
    "    data = go.Bar(\n",
    "                x=words[0:100],\n",
    "                y=times[0:100]\n",
    "        )\n",
    "\n",
    "    datas.append(data)\n",
    "    #layouts.append(layouts)\n",
    "fig = tools.make_subplots(rows=4, cols=2,  \n",
    "                          specs=[[{}, {}],[{}, {}],[{}, {}], [{'colspan': 2}, None]], \n",
    "                          subplot_titles=(books[0], books[1], books[2],\n",
    "                          books[3],books[4],books[5],books[6]))\n",
    "fig.append_trace(datas[0],1,1)\n",
    "fig.append_trace(datas[1],1,2)\n",
    "fig.append_trace(datas[2],2,1)\n",
    "fig.append_trace(datas[3],2,2)\n",
    "fig.append_trace(datas[4],3,1)\n",
    "fig.append_trace(datas[5],3,2)\n",
    "fig.append_trace(datas[6],4,1)\n",
    "\n",
    "for i in range(len(books)):\n",
    "    fig['layout']['xaxis'+str(i+1)].update(title='Word')\n",
    "    \n",
    "for j in range(len(books)):\n",
    "    fig['layout']['yaxis'+str(j+1)].update(title='Frequency')\n",
    "\n",
    "fig['layout'].update(showlegend= False)\n",
    "\n",
    "fig['layout'].update(height=1000, width=1000, \n",
    "                     title=\"Zip's law in all Harry Potter books\")\n",
    "py.iplot(fig, filename='basic-bar')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore the building blocks that will allow us to determine the importance of a word/phrase to a corpus using the Vector Space Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency (TF) is the number of times a word appears in a given corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the term frequency of \"dope\" in \"That was dope.\" is 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse document frequency (IDF) is a measure of the rareness of a word in a corpus. It is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $idf_{t}=log(\\frac{N}{t_f})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where N is the number of documents and $t_f$ is the number of documents in the corpus which the term is present in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"middle\">\n",
       "<video width=\"100%\" autoplay loop>\n",
       "      <source src=\"VSM.mp4\" type=\"video/mp4\">\n",
       "</video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"100%\" autoplay loop>\n",
    "      <source src=\"VSM.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")\n",
    "#See attached video VSM.mp4 if unable to view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, we can see that when the term isn't in that many documents, the input to the log function will be high, since N is constant, making the output higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is the term freqency multiplied by the inverse document frequency. This is a measure of the uniqueness of a word in a document that exists in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf-idf = tf*idf$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf-idf values are then put into a vector that represents the word or document in a vector space. The cosine similarity is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Cosine\\space Similarity = \\frac{a \\cdot b}{\\|a \\|\\|b \\|}$ where a and b are two document or word/phrase vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"middle\">\n",
       "<video width=\"100%\" autoplay loop>\n",
       "      <source src=\"CS.mp4\" type=\"video/mp4\">\n",
       "</video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"100%\" autoplay loop>\n",
    "      <source src=\"CS.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")\n",
    "#See attached video CS.mp4 if unable to view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the cosine similarity, the greater the dot product, since they are more in the same direction i.e. the projection of one vector onto the other is large. Each vector is divided by its magnitude to normalize the distance since we only care about the angle between them, so a unit vector is created in the direction of each vector. The cosine similarity is equal to $cos(\\theta)$, shown in the example above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text files for books was obtained from http://www.glozman.com/textpages.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'Potter', 'and', 'the', 'Sorcerer', \"'s\", 'Stone', 'CHAPTER', 'ONE', 'THE']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "file = open(\"B1.txt\", 'rt')\n",
    "text = file.read()\n",
    "text=nltk.word_tokenize(text)\n",
    "print(text[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like it is being tokenzied properly. Now, we should find the tf-idf vectors, find the cosine similarity between each of the queries and books, get the screen times of each character in each movie, and see if there is a correlation between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HARRY POTTER</th>\n",
       "      <th>RON WEASLEY</th>\n",
       "      <th>HERMIONE GRANGER</th>\n",
       "      <th>VOLDEMORT</th>\n",
       "      <th>DUMBLEDORE</th>\n",
       "      <th>HAGRID</th>\n",
       "      <th>DRACO MALFOY</th>\n",
       "      <th>SIRIUS BLACK</th>\n",
       "      <th>NEVILLE LONGBOTTOM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SORCERER STONE</th>\n",
       "      <td>141.351</td>\n",
       "      <td>47.602</td>\n",
       "      <td>29.664</td>\n",
       "      <td>5.436</td>\n",
       "      <td>22.853</td>\n",
       "      <td>53.403</td>\n",
       "      <td>13.619</td>\n",
       "      <td>5.867</td>\n",
       "      <td>12.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAMBER OF SECRETS</th>\n",
       "      <td>159.369</td>\n",
       "      <td>78.557</td>\n",
       "      <td>31.224</td>\n",
       "      <td>3.247</td>\n",
       "      <td>20.357</td>\n",
       "      <td>21.093</td>\n",
       "      <td>23.424</td>\n",
       "      <td>5.478</td>\n",
       "      <td>3.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRISONER OF AZKABAN</th>\n",
       "      <td>149.567</td>\n",
       "      <td>68.196</td>\n",
       "      <td>44.165</td>\n",
       "      <td>3.532</td>\n",
       "      <td>11.281</td>\n",
       "      <td>27.362</td>\n",
       "      <td>16.343</td>\n",
       "      <td>31.253</td>\n",
       "      <td>11.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOBLET OF FIRE</th>\n",
       "      <td>103.896</td>\n",
       "      <td>91.440</td>\n",
       "      <td>29.137</td>\n",
       "      <td>6.507</td>\n",
       "      <td>3.149</td>\n",
       "      <td>1.459</td>\n",
       "      <td>8.282</td>\n",
       "      <td>10.485</td>\n",
       "      <td>1.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORDER OF THE PHOENIX</th>\n",
       "      <td>128.195</td>\n",
       "      <td>63.199</td>\n",
       "      <td>39.236</td>\n",
       "      <td>5.911</td>\n",
       "      <td>28.657</td>\n",
       "      <td>16.230</td>\n",
       "      <td>6.455</td>\n",
       "      <td>24.405</td>\n",
       "      <td>7.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HALF-BLOOD PRINCE</th>\n",
       "      <td>130.245</td>\n",
       "      <td>49.585</td>\n",
       "      <td>32.546</td>\n",
       "      <td>15.609</td>\n",
       "      <td>66.981</td>\n",
       "      <td>15.092</td>\n",
       "      <td>20.805</td>\n",
       "      <td>7.490</td>\n",
       "      <td>3.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEATHLY HALLOWS (PART 1 AND 2)</th>\n",
       "      <td>124.315</td>\n",
       "      <td>60.461</td>\n",
       "      <td>54.717</td>\n",
       "      <td>16.536</td>\n",
       "      <td>33.336</td>\n",
       "      <td>11.902</td>\n",
       "      <td>1.443</td>\n",
       "      <td>7.959</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                HARRY POTTER  RON WEASLEY  HERMIONE GRANGER  \\\n",
       "SORCERER STONE                       141.351       47.602            29.664   \n",
       "CHAMBER OF SECRETS                   159.369       78.557            31.224   \n",
       "PRISONER OF AZKABAN                  149.567       68.196            44.165   \n",
       "GOBLET OF FIRE                       103.896       91.440            29.137   \n",
       "ORDER OF THE PHOENIX                 128.195       63.199            39.236   \n",
       "HALF-BLOOD PRINCE                    130.245       49.585            32.546   \n",
       "DEATHLY HALLOWS (PART 1 AND 2)       124.315       60.461            54.717   \n",
       "\n",
       "                                VOLDEMORT  DUMBLEDORE  HAGRID  DRACO MALFOY  \\\n",
       "SORCERER STONE                      5.436      22.853  53.403        13.619   \n",
       "CHAMBER OF SECRETS                  3.247      20.357  21.093        23.424   \n",
       "PRISONER OF AZKABAN                 3.532      11.281  27.362        16.343   \n",
       "GOBLET OF FIRE                      6.507       3.149   1.459         8.282   \n",
       "ORDER OF THE PHOENIX                5.911      28.657  16.230         6.455   \n",
       "HALF-BLOOD PRINCE                  15.609      66.981  15.092        20.805   \n",
       "DEATHLY HALLOWS (PART 1 AND 2)     16.536      33.336  11.902         1.443   \n",
       "\n",
       "                                SIRIUS BLACK  NEVILLE LONGBOTTOM  \n",
       "SORCERER STONE                         5.867              12.972  \n",
       "CHAMBER OF SECRETS                     5.478               3.831  \n",
       "PRISONER OF AZKABAN                   31.253              11.076  \n",
       "GOBLET OF FIRE                        10.485               1.214  \n",
       "ORDER OF THE PHOENIX                  24.405               7.192  \n",
       "HALF-BLOOD PRINCE                      7.490               3.825  \n",
       "DEATHLY HALLOWS (PART 1 AND 2)         7.959               0.435  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=[]\n",
    "books = [\n",
    "    \"SORCERER STONE\",\n",
    "    \"CHAMBER OF SECRETS\",\n",
    "    \"PRISONER OF AZKABAN\",\n",
    "    \"GOBLET OF FIRE\",\n",
    "    \"ORDER OF THE PHOENIX\",\n",
    "    \"HALF-BLOOD PRINCE\",\n",
    "    \"DEATHLY HALLOWS (PART 1 AND 2)\"\n",
    "]\n",
    "num_books = 7\n",
    "for i in range(num_books):\n",
    "        file = open(f\"B{i+1}.txt\", 'rt')\n",
    "        text = file.read()\n",
    "        corpus.append(text)\n",
    "        \n",
    "characters=[\n",
    "    \"HARRY POTTER\",\n",
    "    \"RON WEASLEY\",\n",
    "    \"HERMIONE GRANGER\",\n",
    "    \"VOLDEMORT\",\n",
    "    \"DUMBLEDORE\",\n",
    "    \"HAGRID\",\n",
    "    \"DRACO MALFOY\",\n",
    "    \"SIRIUS BLACK\",\n",
    "    \"NEVILLE LONGBOTTOM\"\n",
    "]\n",
    "\n",
    "cosine_similarities={}\n",
    "for character in characters:\n",
    "    corpus.append(character)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus) \n",
    "    character_sim=[]\n",
    "    for i in range(num_books):\n",
    "        similarity = float(cosine_similarity(tfidf_matrix[i],tfidf_matrix[-1]))*1000\n",
    "        sim_rounded = round(similarity,3)\n",
    "        character_sim.append(sim_rounded)\n",
    "    cosine_similarities[character]=character_sim\n",
    "\n",
    "pd.DataFrame(cosine_similarities, index=books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the cosine similarity was multiplied by 1000 for easier viewing and distinction between the values. All calculations past this point will be based off the multiplied cosine similarity values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get screen times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using IMDB's movie database (https://www.imdb.com/list/ls027460372/), we can get the screen times for each of the actors in the films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HARRY POTTER</th>\n",
       "      <th>RON WEASLEY</th>\n",
       "      <th>HERMIONE GRANGER</th>\n",
       "      <th>VOLDEMORT</th>\n",
       "      <th>DUMBLEDORE</th>\n",
       "      <th>HAGRID</th>\n",
       "      <th>DRACO MALFOY</th>\n",
       "      <th>SIRIUS BLACK</th>\n",
       "      <th>NEVILLE LONGBOTTOM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SORCERER STONE</th>\n",
       "      <td>72.75</td>\n",
       "      <td>28.25</td>\n",
       "      <td>23.25</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.75</td>\n",
       "      <td>16.50</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAMBER OF SECRETS</th>\n",
       "      <td>83.75</td>\n",
       "      <td>38.25</td>\n",
       "      <td>15.50</td>\n",
       "      <td>6.75</td>\n",
       "      <td>10.75</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRISONER OF AZKABAN</th>\n",
       "      <td>74.50</td>\n",
       "      <td>21.25</td>\n",
       "      <td>34.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.25</td>\n",
       "      <td>5.25</td>\n",
       "      <td>4.00</td>\n",
       "      <td>11.50</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOBLET OF FIRE</th>\n",
       "      <td>63.25</td>\n",
       "      <td>20.50</td>\n",
       "      <td>16.50</td>\n",
       "      <td>6.50</td>\n",
       "      <td>14.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORDER OF THE PHOENIX</th>\n",
       "      <td>61.75</td>\n",
       "      <td>21.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>2.25</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>7.50</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HALF-BLOOD PRINCE</th>\n",
       "      <td>67.00</td>\n",
       "      <td>21.75</td>\n",
       "      <td>20.00</td>\n",
       "      <td>4.25</td>\n",
       "      <td>22.25</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEATHLY HALLOWS (PART 1 AND 2)</th>\n",
       "      <td>58.13</td>\n",
       "      <td>30.38</td>\n",
       "      <td>36.00</td>\n",
       "      <td>7.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                HARRY POTTER  RON WEASLEY  HERMIONE GRANGER  \\\n",
       "SORCERER STONE                         72.75        28.25             23.25   \n",
       "CHAMBER OF SECRETS                     83.75        38.25             15.50   \n",
       "PRISONER OF AZKABAN                    74.50        21.25             34.75   \n",
       "GOBLET OF FIRE                         63.25        20.50             16.50   \n",
       "ORDER OF THE PHOENIX                   61.75        21.00             23.00   \n",
       "HALF-BLOOD PRINCE                      67.00        21.75             20.00   \n",
       "DEATHLY HALLOWS (PART 1 AND 2)         58.13        30.38             36.00   \n",
       "\n",
       "                                VOLDEMORT  DUMBLEDORE  HAGRID  DRACO MALFOY  \\\n",
       "SORCERER STONE                       2.00        9.75   16.50          4.25   \n",
       "CHAMBER OF SECRETS                   6.75       10.75    8.25          7.00   \n",
       "PRISONER OF AZKABAN                  0.00        6.25    5.25          4.00   \n",
       "GOBLET OF FIRE                       6.50       14.25    3.75          2.25   \n",
       "ORDER OF THE PHOENIX                 2.25        7.50    2.75          1.25   \n",
       "HALF-BLOOD PRINCE                    4.25       22.25    4.00          8.25   \n",
       "DEATHLY HALLOWS (PART 1 AND 2)       7.75        3.25    2.63          2.33   \n",
       "\n",
       "                                SIRIUS BLACK  NEVILLE LONGBOTTOM  \n",
       "SORCERER STONE                          0.00                3.25  \n",
       "CHAMBER OF SECRETS                      0.00                1.25  \n",
       "PRISONER OF AZKABAN                    11.50                3.00  \n",
       "GOBLET OF FIRE                          1.25                4.25  \n",
       "ORDER OF THE PHOENIX                    7.50                9.00  \n",
       "HALF-BLOOD PRINCE                       0.00                1.50  \n",
       "DEATHLY HALLOWS (PART 1 AND 2)          0.00                0.12  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#screen times are in order of the books as shown above\n",
    "screen_time={\"HARRY POTTER\":[72.75,83.75,74.5,63.25,61.75, 67.00,round(58+8/60,2)],\n",
    "             \"RON WEASLEY\":[28.25,38.25, 21.25,20.5,21.0,21.75,round(30+23/60,2)],\n",
    "             \"HERMIONE GRANGER\":[23.25,15.5,34.75,16.5,23,20,36],\n",
    "             \"VOLDEMORT\":[2,6.75,0,6.5,2.25,4.25,7.75],\n",
    "             \"DUMBLEDORE\":[9.75,10.75,6.25,14.25,7.5,22.25,3.25],\n",
    "             \"HAGRID\":[16.5,8.25,5.25,3.75,2.75,4,round(2+38/60,2)],\n",
    "             \"DRACO MALFOY\":[4.25,7,4,2.25,1.25,8.25,2.33],\n",
    "             \"SIRIUS BLACK\":[0,0,11.5,1.25,7.5,0,0],\n",
    "             \"NEVILLE LONGBOTTOM\":[3.25,1.25,3,4.25,9,1.5,round(7/60,2)]\n",
    "            }\n",
    "pd.DataFrame(screen_time, index=books) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get average cosine similarity and screen time for each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"middle\">\n",
       "<img width=\"75%\" src=\"average.png\">\n",
       "</img></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_similarities = {}\n",
    "average_screen_times = {}\n",
    "for character in characters:\n",
    "    average_similarities[character]=sum(cosine_similarities[character])\\\n",
    "    /len(cosine_similarities[character])\n",
    "    average_screen_times[character]=sum(screen_time[character])\\\n",
    "    /len(screen_time[character])\n",
    "    \n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<img width=\"75%\" src=\"average.png\">\n",
    "</img></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot on graph to see relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jskauf/117.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "sims = np.array(list((average_similarities.values())))\n",
    "times = np.array(list((average_screen_times.values())))\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(sims,times)\n",
    "line = slope*sims+intercept\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = sims,\n",
    "    y = times,\n",
    "    mode = 'markers',\n",
    "    text = characters\n",
    ")\n",
    "\n",
    "trace2 = trace2 = go.Scatter(\n",
    "                  x=sims,\n",
    "                  y=line,\n",
    "                  mode='lines',\n",
    "                  marker=go.Marker(color='rgb(31, 119, 180)'),\n",
    "                  name='Fit'\n",
    "                  )\n",
    "\n",
    "layout= go.Layout(\n",
    "    title= 'Average screen time vs cosine similarity \\\n",
    "    for harry potter characters over all books and movies',\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title= 'Cosine Similarity',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Screen Time (minutes)',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "data = [trace1, trace2]\n",
    "fig= go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "py.iplot(fig, filename='basic-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the graph above, there is a strong positive correlation between cosine similarity and screen time. (more discussed in conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levels of Linguistic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word and phrase creation is a difficult task. To make it easier on ourselves, we formulate phrases through a combination of meaningful words and connecting words. This gives us time to think without pausing too often. Beniot Mandelbrot, a famous mathemetician, said that nature always looks for the path of least energy and language may have evolved in a similar process in order to keep language as fluid and understandable as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explored above, the meaning of speech and writing is held in a select few words. These words don't occur often, but that is what makes them influential. The downside to this is that meaning takes long to convey. The Zipf-ian nature of language seems to have caused verboseness to be an inherent part of our language. If every word we said had equally influential meaning, then communication could occur much more efficiently. The question is whether the human brain would be capable of producing coherent information that quickly. Unfortunatly, that is probably not the case. Moreover, for the listener, the meaning is not always easy to find since it exists in a sea of meaningless words. You have to be constantly listening for the important parts because if you lose focus for a brief time, you may miss them. In all, it would make sense for language to evolve for ease of production and retention, but that doesn't seem to be the case. It may be that we are in the nascent stages of language and it may take longer for it to reach its maximum potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, I was attempting to uncover the interlinked nature of literature and film through analyzing the content of the Harry Potter books. More specifically, I wanted to see how well the characters in the books were depicted on-screen. To do this, I used the cosine similarity of a character’s name and each book as a proxy for the relative importance of the term. I believe this is a fair assumption because, many times, the use of a name indicates that the character had a line of dialogue, which directly relates to them being onscreen. In addition, the use of a character’s name in any regard works to increase their importance in the overall scheme of the book as it shows they are more integral to the plot. My hypothesis was that the higher the cosine similarity (loosely translated as the importance of the name in the book), the higher the screen time for that character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment confirmed my hypothesis and showed that the importance of the characters in the books was accurately portrayed through the film. As seen from the graph above, the relationship is linear with an $r^2$  value of 0.974, which indicates a strong positive correlation. This implies that the movie closely mirrored the events in the books and each character’s contributions. Interestingly, as much as Voldemort is a major component of the plot of many of the films, his screen time doesn’t correlate that well with the weight of his name in each of the books. This makes sense though as Voldemort is talked about a lot by other characters, increasing his cosine-similarity, but only present during the tension and action filled scenes towards the end of films. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all, we explored how Zipf's law characterizes language, allowing for the Vector Space Model to be used effectively, and how different expressions of language - books and movies - relate to one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using manim math engine, code for visualization of log function\n",
    "class LOG(GraphScene):\n",
    "    CONFIG = {\n",
    "    \"x_min\" : -5,\n",
    "    \"x_max\" : 5,\n",
    "    \"y_min\" : 0,\n",
    "    \"y_max\" : 25,\n",
    "    \"graph_origin\" : DOWN*3,\n",
    "    \"function_color\" : RED,\n",
    "    \"axes_color\" : GREEN,\n",
    "    \"x_axis_label\": None,\n",
    "    \"y_axis_label\": None,\n",
    "    }\n",
    "\n",
    "    def construct(self):\n",
    "        self.setup_axes(animate=True)\n",
    "        graph = self.get_graph(lambda t : np.exp(t),color=self.function_color)\n",
    "        self.play(ShowCreation(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using manim math engine, code for visualization of cosine similarity\n",
    "class CS(Scene):\n",
    "    CONFIG = {\n",
    "    \"x_min\" : -10,\n",
    "    \"x_max\" : 10,\n",
    "    \"y_min\" : -10,\n",
    "    \"y_max\" : 10,\n",
    "    \"function_color\" : RED,\n",
    "    \"axes_color\" : GREEN,\n",
    "    }\n",
    "\n",
    "    def construct(self):\n",
    "        plane = NumberPlane()\n",
    "        vector1=Vector(3*UP+RIGHT)\n",
    "        vector2=Vector(3*RIGHT+UP)\n",
    "        group=VGroup(vector1,vector2)\n",
    "        x_line=Line(ORIGIN,RIGHT, color=RED)\n",
    "        y_line=Line(RIGHT,3*UP+RIGHT, color=GREEN)\n",
    "        ordered_pair = TexMobject(\"[1,3]\")\n",
    "        ordered_pair.next_to(vector1.get_tip())\n",
    "        self.play(ShowCreation(\n",
    "            plane, \n",
    "            submobject_mode = \"lagged_start\",\n",
    "            run_time = 3\n",
    "        ))\n",
    "        self.play(GrowFromPoint(\n",
    "            group,\n",
    "            ORIGIN,\n",
    "            submobject_mode = \"lagged_start\"),\n",
    "            run_time=3)\n",
    "        self.wait()\n",
    "        self.play(ShowCreation(\n",
    "            x_line\n",
    "        ))\n",
    "        self.play(ShowCreation(\n",
    "            y_line\n",
    "        ))\n",
    "        self.play(Write(\n",
    "            ordered_pair\n",
    "        ))\n",
    "        self.play(FadeOut(\n",
    "            x_line\n",
    "        ),FadeOut(\n",
    "            y_line\n",
    "        ))\n",
    "        x_line=Line(ORIGIN,3*RIGHT, color=RED)\n",
    "        y_line=Line(3*RIGHT,3*RIGHT+UP, color=GREEN)\n",
    "        ordered_pair_2 = TexMobject(\"[3,1]\")\n",
    "        ordered_pair_2.next_to(vector2.get_tip())\n",
    "        self.play(ShowCreation(\n",
    "            x_line\n",
    "        )) \n",
    "        self.play(ShowCreation(\n",
    "            y_line\n",
    "        ))\n",
    "        self.play(Write(\n",
    "            ordered_pair_2\n",
    "        ))\n",
    "        self.play(FadeOut(\n",
    "            x_line\n",
    "        ),FadeOut(\n",
    "            y_line\n",
    "        ))\n",
    "        theta = Arc(np.arctan(3)-np.arctan(1/3),start_angle=np.arctan(1/3))\n",
    "        angle = TexMobject(\"\\\\theta\")\n",
    "        angle.next_to(theta)\n",
    "        angle.shift(UP*.3+LEFT*.3)\n",
    "        equation = TexMobject(\"cos(\\\\theta)=\\\\frac{a\\\\cdot b}{||a|| \\\\times ||b||}\")\n",
    "        equation.shift(2.5*LEFT+2*DOWN)\n",
    "        equation_2 = TexMobject(\"cos(\\\\theta)=\\\\frac{[1,3]\\\\cdot \\\n",
    "                                [3,1]}{||[1,3]|| \\\\times ||[3,1]||}\")\n",
    "        equation_2.shift(2.5*LEFT+2*DOWN)\n",
    "        equation_3 = TexMobject(\"cos(\\\\theta)=\\\n",
    "                                \\\\frac{1\\\\times 3 + 3 \\\\times 1}{\\\\sqrt{1^2+3^2} \\\n",
    "                                \\\\times \\\\sqrt{3^2+1^2}}\")\n",
    "        equation_3.shift(2.5*LEFT+2*DOWN)\n",
    "        equation_4 = TexMobject(\"cos(\\\\theta)=\\\\frac{6}{\\\\sqrt{10} \\\\times \\\\sqrt{10}\")\n",
    "        equation_4.shift(2.5*LEFT+2*DOWN)\n",
    "        equation_5 = TexMobject(\"cos(\\\\theta)=\\\\frac{3}{5}=Cosine \\\\hspace{1.5mm} Similarity\")\n",
    "        equation_5.shift(2.5*LEFT+2*DOWN)\n",
    "        self.play(\n",
    "            ShowCreation(theta),\n",
    "            ShowCreation(angle)\n",
    "        )\n",
    "        self.play(Write(equation))\n",
    "        self.wait()\n",
    "        self.play(FocusOn(ordered_pair),FocusOn(ordered_pair_2))\n",
    "        self.wait()\n",
    "        self.play(FadeOut(equation),Write(equation_2), run_time=2)\n",
    "        self.wait()\n",
    "        self.play(FadeOut(equation_2),Write(equation_3), run_time=2)\n",
    "        self.wait()\n",
    "        self.play(FadeOut(equation_3),Write(equation_4), run_time=2)\n",
    "        self.wait()\n",
    "        self.play(FadeOut(equation_4),Write(equation_5), run_time=2)\n",
    "        self.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
